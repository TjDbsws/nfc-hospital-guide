---
description: 
globs: ai-server/**/*.py, python/**/*.py, **/*_model.py, **/*_ai.py
alwaysApply: false
---
# Python AI Server Development Rules

**파일 패턴**: `ai-server/**/*.py`, `python/**/*.py`, `**/*_model.py`, `**/*_ai.py`
**적용 조건**: Python Flask AI 서버 및 머신러닝 모델 개발 시

**도움이 되는 작업**: AI 챗봇 구현, LSTM 대기시간 예측, BERT 자연어 처리, Flask API 개발, 모델 학습 및 추론

## 1. Flask 서버 기본 구조

### 메인 앱 설정
```python
# app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import logging
from datetime import datetime
import jwt

app = Flask(__name__)
CORS(app, origins=["http://localhost:3000"])  # Node.js 서버만 허용

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(name)s %(message)s'
)
logger = logging.getLogger(__name__)

# 환경변수 설정
app.config['JWT_SECRET'] = os.getenv('JWT_SECRET')
app.config['MODEL_PATH'] = os.getenv('MODEL_PATH', './models')

# Blueprint 등록
from routes.chatbot import chatbot_bp
from routes.prediction import prediction_bp

app.register_blueprint(chatbot_bp, url_prefix='/api/v1/chatbot')
app.register_blueprint(prediction_bp, url_prefix='/api/v1/prediction')

# 표준 응답 포맷
def create_response(success=True, data=None, message='', error=None):
    response = {
        'success': success,
        'timestamp': datetime.now().isoformat()
    }
    
    if success:
        response['data'] = data
        if message:
            response['message'] = message
    else:
        response['error'] = error
    
    return response

# 에러 핸들러
@app.errorhandler(Exception)
def handle_error(error):
    logger.error(f"Unexpected error: {str(error)}")
    return jsonify(create_response(
        success=False,
        error={
            'code': 'INTERNAL_ERROR',
            'message': '서버 내부 오류가 발생했습니다.'
        }
    )), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

### JWT 인증 데코레이터
```python
# middleware/auth.py
from functools import wraps
from flask import request, jsonify, current_app
import jwt

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = request.headers.get('Authorization')
        
        if not token:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'UNAUTHORIZED',
                    'message': '인증 토큰이 필요합니다.'
                }
            }), 401
        
        try:
            # "Bearer TOKEN" 형식에서 토큰 추출
            token = token.split(' ')[1]
            data = jwt.decode(token, current_app.config['JWT_SECRET'], algorithms=['HS256'])
            current_user = data
        except jwt.ExpiredSignatureError:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'TOKEN_EXPIRED',
                    'message': '토큰이 만료되었습니다.'
                }
            }), 401
        except jwt.InvalidTokenError:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'INVALID_TOKEN',
                    'message': '유효하지 않은 토큰입니다.'
                }
            }), 401
        
        return f(current_user, *args, **kwargs)
    
    return decorated
```

## 2. AI 챗봇 구현 (BERT 기반)

### BERT 모델 로더
```python
# models/chatbot_model.py
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle

class MedicalChatbot:
    def __init__(self, model_path='./models'):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.faq_data = None
        self.intent_model = None
        self.load_models()
    
    def load_models(self):
        """BERT 모델과 FAQ 데이터 로드"""
        try:
            # BERT 토크나이저와 모델 로드
            self.tokenizer = BertTokenizer.from_pretrained('klue/bert-base')
            self.model = BertForSequenceClassification.from_pretrained(
                self.model_path + '/bert_medical'
            )
            
            # FAQ 데이터 로드
            with open(f'{self.model_path}/faq_data.json', 'r', encoding='utf-8') as f:
                self.faq_data = json.load(f)
            
            # 인텐트 분류 모델 로드
            with open(f'{self.model_path}/intent_classifier.pkl', 'rb') as f:
                self.intent_model = pickle.load(f)
                
            print("모델 로딩 완료")
            
        except Exception as e:
            print(f"모델 로딩 실패: {e}")
            # 기본 응답 모드로 전환
            self.setup_fallback_mode()
    
    def setup_fallback_mode(self):
        """모델 로딩 실패 시 기본 FAQ 응답 모드"""
        self.faq_data = {
            "기본": {
                "patterns": ["안녕", "도움", "문의"],
                "responses": ["안녕하세요. 병원 안내 챗봇입니다. 무엇을 도와드릴까요?"]
            },
            "위치": {
                "patterns": ["어디", "위치", "장소", "길"],
                "responses": ["위치 관련 문의는 NFC 태그를 스캔하거나 안내데스크에 문의해주세요."]
            },
            "검사": {
                "patterns": ["검사", "진료", "예약", "시간"],
                "responses": ["검사 관련 정보는 해당 검사실 NFC 태그를 스캔하시면 확인하실 수 있습니다."]
            }
        }
    
    def preprocess_text(self, text):
        """텍스트 전처리"""
        # 기본 전처리: 공백 제거, 소문자 변환 등
        text = text.strip().lower()
        # 불필요한 문자 제거
        import re
        text = re.sub(r'[^\w\s]', '', text)
        return text
    
    def classify_intent(self, text):
        """사용자 의도 분류"""
        text = self.preprocess_text(text)
        
        # 키워드 기반 의도 분류 (fallback)
        if any(keyword in text for keyword in ['어디', '위치', '장소', '길']):
            return 'location'
        elif any(keyword in text for keyword in ['검사', '진료', '예약']):
            return 'exam'
        elif any(keyword in text for keyword in ['시간', '대기', '순서']):
            return 'waiting'
        elif any(keyword in text for keyword in ['준비', '금식', '주의']):
            return 'preparation'
        else:
            return 'general'
    
    def find_best_response(self, text, intent, context=None):
        """최적 응답 찾기"""
        text = self.preprocess_text(text)
        
        # 컨텍스트 기반 응답 (검사실 위치, 환자 상태 등)
        if context:
            if intent == 'exam' and context.get('current_exam'):
                exam_info = context['current_exam']
                return f"{exam_info['title']} 검사에 대한 정보입니다. 소요시간은 약 {exam_info['duration']}분입니다."
            
            if intent == 'waiting' and context.get('queue_status'):
                queue_info = context['queue_status']
                return f"현재 대기 순서는 {queue_info['position']}번이고, 예상 대기시간은 {queue_info['estimated_time']}분입니다."
        
        # FAQ 기반 응답
        best_match = None
        best_score = 0
        
        for category, data in self.faq_data.items():
            for pattern in data['patterns']:
                # 간단한 키워드 매칭
                if pattern in text:
                    score = len(pattern) / len(text)
                    if score > best_score:
                        best_score = score
                        best_match = data['responses'][0]
        
        if best_match:
            return best_match
        
        # 기본 응답
        return "죄송합니다. 정확한 답변을 드리기 어렵습니다. 안내데스크에 문의해주세요."
    
    def get_response(self, user_input, context=None):
        """사용자 입력에 대한 응답 생성"""
        try:
            intent = self.classify_intent(user_input)
            response = self.find_best_response(user_input, intent, context)
            
            return {
                'type': 'faq_answer',
                'content': response,
                'confidence': 0.8,  # 간단한 신뢰도 점수
                'intent': intent,
                'sources': ['faq_database']
            }
            
        except Exception as e:
            print(f"응답 생성 오류: {e}")
            return {
                'type': 'error',
                'content': '응답 생성 중 오류가 발생했습니다. 잠시 후 다시 시도해주세요.',
                'confidence': 0.0,
                'intent': 'error',
                'sources': []
            }

# 전역 챗봇 인스턴스
chatbot = MedicalChatbot()
```

### 챗봇 라우터
```python
# routes/chatbot.py
from flask import Blueprint, request, jsonify
from models.chatbot_model import chatbot
from middleware.auth import token_required

chatbot_bp = Blueprint('chatbot', __name__)

@chatbot_bp.route('/query', methods=['POST'])
@token_required
def handle_text_query(current_user):
    """텍스트 질문 처리"""
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        context = data.get('context', {})
        
        if not question:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'INVALID_REQUEST',
                    'message': '질문을 입력해주세요.'
                }
            }), 400
        
        # 챗봇 응답 생성
        response = chatbot.get_response(question, context)
        
        # 응답 로깅 (개인정보 제외)
        import logging
        logging.info(f"Chatbot query - User: {current_user.get('userId')}, Intent: {response.get('intent')}")
        
        return jsonify({
            'success': True,
            'data': {
                'query': question,
                'response': response,
                'timestamp': datetime.now().isoformat()
            },
            'message': '응답이 생성되었습니다.'
        })
        
    except Exception as e:
        logging.error(f"Chatbot query error: {str(e)}")
        return jsonify({
            'success': False,
            'error': {
                'code': 'INTERNAL_ERROR',
                'message': '챗봇 응답 생성 중 오류가 발생했습니다.'
            }
        }), 500

@chatbot_bp.route('/voice-query', methods=['POST'])
@token_required
def handle_voice_query(current_user):
    """음성 질문 처리"""
    try:
        data = request.get_json()
        audio_data = data.get('audioData')  # base64 encoded
        audio_format = data.get('format', 'wav')
        
        if not audio_data:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'INVALID_REQUEST',
                    'message': '음성 데이터가 필요합니다.'
                }
            }), 400
        
        # 음성을 텍스트로 변환 (STT)
        text = speech_to_text(audio_data, audio_format)
        
        if not text:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'STT_FAILED',
                    'message': '음성 인식에 실패했습니다.'
                }
            }), 400
        
        # 텍스트 질문 처리
        context = data.get('context', {})
        response = chatbot.get_response(text, context)
        
        return jsonify({
            'success': True,
            'data': {
                'recognizedText': text,
                'response': response,
                'timestamp': datetime.now().isoformat()
            },
            'message': '음성 질문이 처리되었습니다.'
        })
        
    except Exception as e:
        logging.error(f"Voice query error: {str(e)}")
        return jsonify({
            'success': False,
            'error': {
                'code': 'INTERNAL_ERROR',
                'message': '음성 질문 처리 중 오류가 발생했습니다.'
            }
        }), 500

def speech_to_text(audio_data, format):
    """음성을 텍스트로 변환하는 함수"""
    # 실제 구현에서는 Google Speech-to-Text API 또는 다른 STT 서비스 사용
    # 여기서는 Mock 구현
    try:
        import base64
        # base64 디코딩 및 음성 처리 로직
        # 실제로는 Google Cloud Speech-to-Text 등 사용
        return "안녕하세요"  # Mock 응답
    except Exception:
        return None
```

## 3. LSTM 대기시간 예측 모델

### LSTM 모델 구현
```python
# models/waiting_time_predictor.py
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import joblib
from datetime import datetime, timedelta
import logging

class WaitingTimePredictor:
    def __init__(self, model_path='./models'):
        self.model_path = model_path
        self.model = None
        self.scaler = None
        self.sequence_length = 60  # 60분간의 데이터로 예측
        self.load_model()
    
    def load_model(self):
        """학습된 LSTM 모델과 스케일러 로드"""
        try:
            self.model = load_model(f'{self.model_path}/lstm_waiting_time.h5')
            self.scaler = joblib.load(f'{self.model_path}/waiting_time_scaler.pkl')
            logging.info("LSTM 모델 로딩 완료")
        except Exception as e:
            logging.error(f"LSTM 모델 로딩 실패: {e}")
            self.setup_fallback_predictor()
    
    def setup_fallback_predictor(self):
        """모델 로딩 실패 시 통계 기반 예측기"""
        # 시간대별 평균 대기시간 (fallback)
        self.hourly_averages = {
            9: 15, 10: 20, 11: 25, 12: 10,  # 오전
            13: 15, 14: 30, 15: 35, 16: 25, 17: 20  # 오후
        }
    
    def prepare_features(self, historical_data):
        """예측을 위한 특성 준비"""
        features = []
        
        # 시간 특성
        current_time = datetime.now()
        features.extend([
            current_time.hour,
            current_time.weekday(),
            current_time.month
        ])
        
        # 대기열 특성
        features.extend([
            historical_data.get('current_queue_length', 0),
            historical_data.get('avg_service_time', 10),
            historical_data.get('current_staff_count', 1)
        ])
        
        # 과거 대기시간 패턴
        past_waiting_times = historical_data.get('past_waiting_times', [])
        if len(past_waiting_times) < self.sequence_length:
            # 부족한 데이터는 평균값으로 패딩
            avg_time = np.mean(past_waiting_times) if past_waiting_times else 15
            past_waiting_times.extend([avg_time] * (self.sequence_length - len(past_waiting_times)))
        
        features.extend(past_waiting_times[-self.sequence_length:])
        
        return np.array(features).reshape(1, -1)
    
    def predict_waiting_time(self, exam_id, historical_data):
        """대기시간 예측"""
        try:
            if self.model is None:
                # Fallback 예측
                return self.fallback_prediction()
            
            # 특성 준비
            features = self.prepare_features(historical_data)
            
            # 스케일링
            features_scaled = self.scaler.transform(features)
            
            # LSTM 입력 형태로 변환
            sequence_data = features_scaled[:, -self.sequence_length:].reshape(1, self.sequence_length, 1)
            
            # 예측
            prediction = self.model.predict(sequence_data)
            predicted_time = self.scaler.inverse_transform(prediction)[0][0]
            
            # 예측값 검증 (음수 방지, 최대값 제한)
            predicted_time = max(1, min(predicted_time, 120))  # 1분~120분
            
            return {
                'predicted_time': int(predicted_time),
                'confidence': self.calculate_confidence(historical_data),
                'method': 'lstm_model'
            }
            
        except Exception as e:
            logging.error(f"LSTM 예측 오류: {e}")
            return self.fallback_prediction()
    
    def fallback_prediction(self):
        """통계 기반 대체 예측"""
        current_hour = datetime.now().hour
        base_time = self.hourly_averages.get(current_hour, 20)
        
        # 약간의 랜덤성 추가
        import random
        variation = random.randint(-5, 5)
        predicted_time = max(5, base_time + variation)
        
        return {
            'predicted_time': predicted_time,
            'confidence': 0.6,
            'method': 'statistical_fallback'
        }
    
    def calculate_confidence(self, historical_data):
        """예측 신뢰도 계산"""
        data_quality = len(historical_data.get('past_waiting_times', []))
        staff_availability = historical_data.get('current_staff_count', 1)
        
        # 간단한 신뢰도 계산 로직
        base_confidence = 0.7
        
        # 데이터 품질에 따른 조정
        if data_quality >= self.sequence_length:
            confidence = base_confidence + 0.2
        elif data_quality >= 30:
            confidence = base_confidence
        else:
            confidence = base_confidence - 0.2
        
        # 직원 수에 따른 조정
        if staff_availability >= 2:
            confidence += 0.1
        
        return min(0.95, max(0.3, confidence))

# 전역 예측기 인스턴스
waiting_predictor = WaitingTimePredictor()
```

### 예측 라우터
```python
# routes/prediction.py
from flask import Blueprint, request, jsonify
from models.waiting_time_predictor import waiting_predictor
from middleware.auth import token_required

prediction_bp = Blueprint('prediction', __name__)

@prediction_bp.route('/waiting-time', methods=['POST'])
@token_required
def predict_waiting_time(current_user):
    """대기시간 예측"""
    try:
        data = request.get_json()
        exam_id = data.get('exam_id')
        historical_data = data.get('historical_data', {})
        
        if not exam_id:
            return jsonify({
                'success': False,
                'error': {
                    'code': 'INVALID_REQUEST',
                    'message': '검사 ID가 필요합니다.'
                }
            }), 400
        
        # 대기시간 예측
        prediction = waiting_predictor.predict_waiting_time(exam_id, historical_data)
        
        return jsonify({
            'success': True,
            'data': {
                'exam_id': exam_id,
                'prediction': prediction,
                'timestamp': datetime.now().isoformat()
            },
            'message': '대기시간 예측이 완료되었습니다.'
        })
        
    except Exception as e:
        logging.error(f"대기시간 예측 오류: {str(e)}")
        return jsonify({
            'success': False,
            'error': {
                'code': 'PREDICTION_ERROR',
                'message': '대기시간 예측 중 오류가 발생했습니다.'
            }
        }), 500
```

## 4. 파일 구조

```
ai-server/
├── models/
│   ├── chatbot_model.py        # BERT 챗봇 모델
│   ├── waiting_time_predictor.py # LSTM 예측 모델
│   └── saved_models/           # 학습된 모델 파일들
│       ├── bert_medical/
│       ├── lstm_waiting_time.h5
│       └── *.pkl
├── routes/
│   ├── chatbot.py             # 챗봇 API 라우터
│   └── prediction.py          # 예측 API 라우터
├── middleware/
│   └── auth.py                # JWT 인증 미들웨어
├── utils/
│   ├── data_preprocessor.py   # 데이터 전처리
│   └── model_trainer.py       # 모델 학습 유틸
├── tests/
│   ├── test_chatbot.py        # 챗봇 테스트
│   └── test_prediction.py     # 예측 테스트
├── requirements.txt           # Python 패키지 목록
├── app.py                     # Flask 앱 메인
└── config.py                  # 설정 파일
```

## 5. 주요 라이브러리

### requirements.txt
```
flask==2.3.0
flask-cors==4.0.0
transformers==4.30.0
torch==2.0.0
tensorflow==2.12.0
scikit-learn==1.2.0
pandas==2.0.0
numpy==1.24.0
pyjwt==2.7.0
python-dotenv==1.0.0
```


이 규칙들을 따라 개발하면 효율적이고 확장 가능한 AI 서버를 구축할 수 있습니다.